# Evaluation Methods

## Automatic Evaluation
We want to first introduce a nice survey paper on text generation evaluations: 
[Evaluation of Text Generation: A Survey](https://arxiv.org/pdf/2006.14799.pdf).
Our idea to include lexical diversity came from this paper.

Due to the speciality of our text generation task, we will use different types of evaluations methods to analyze
different aspects of our lyrics generation.

### 1. Grammar Check 
function: check_grammar

We first want to check if the generated line of lyrics have relatively low percentage of grammar errors.
While we don't expect lyrics to be completely correct in grammar, we do hope that the number of mistakes 
relative to sentence length to be small. We use the package introduced here:
https://pypi.org/project/language-tool-python/
and calculate the number of errors/number of sentence length in terms of characters.
**This function thus will return a score - the lower the better.**
```commandline
pip install language-tool-python
```

### 2. Rhyme Check
function: check_rhyme_phyme

We expect to have models that enforce rhymes for each line of lyrics, and models that does not enforce this.
For models that does not enforce rhymesï¼Œwe will evaluate if the generated next sentence's last word will rhyme
with previous sentence's last word. We used a great package called [```Phyme```](https://github.com/jameswenzel/Phyme), 
which is build based on [```CMU Pronouncing Dictionary```](http://www.speech.cs.cmu.edu/cgi-bin/cmudict). 
The package has several types of rhymes: family, perfect, additive, subtractive, substitution, assonance, 
and consonant. We take a set of all rhyme words generated by ```Phyme``` including all types except for consonant. 
This is because for consonant, rhyme words includes "grace" and "guys", which does not produce the in [i:] in [ni:s].
We do want the words to have the same vowels ideally. We then see if the generated text's last word is in the set of 
rhymes for the previous sentence's last word.**This function thus will return a boolean value - True means that the 
next sentence line rhymes with the previous line.**


In this method, we catch a few caveats and edge cases. 
* We remove all punctuations except apostrophes using regular expression.
  
* First, we considered cases that ends with a numeric number, for example 8, 21, etc.
  In these cases, we would use [num2word](https://pypi.org/project/num2words/) package to convert the input last word to
  english words like eight, twenty-one.
  
* Second, in rare cases, we use the grammar check mentioned above to check if the input text's last word has 
  unconventional spellings; if it does, we correct it to make sure it can be found in the 
  [rhyme dictionary](https://github.com/jameswenzel/Phyme/blob/master/Phyme/data/word_phone.json).
  We checked that words like "y'all" and "somethin'" are in the dictionary, which shows the package is robust to such
  common spoken words.


```commandline
pip install phyme
```

### 3. BLEU, ROUGE, and BERT Score
BLEU function: get_bleu_score

ROUGE function: get_summary_rouge_score

BERT Score function: get_summary_bert_score

These three methods are similar in that they evaluate if the candidate sentence is similar to the reference sentence(s).

The ```get_bleu_score``` function uses [```nltk.translate.bleu_score```](https://www.nltk.org/_modules/nltk/translate/bleu_score.html) 
and allow inputs such as ngram_order, weights, smoothing function, 
auto_reweigh, and use_token. Given our generated next sentence/lyrics is likely to be short, we would use a lower 
ngram_order, and possibly use a [smoothing function](http://acl2014.org/acl2014/W14-33/pdf/W14-3346.pdf) of 
[```SmoothingFunction().method4```](https://www.nltk.org/_modules/nltk/translate/bleu_score.html).

```commandline
pip install nltk
```

The ```get_summary_rouge_score``` function uses package [```rouge```](https://pypi.org/project/rouge/), and includes 
unigram, bigram, or LCS (Longest Common Subsequence) evaluations. Rouge only compares candidate with one reference, 
so we made the function to compute summary scores (mean, max, min) if there are multiple references.

```commandline
pip install rouge
```

The  ```get_bert_score``` function uses [```bert_score```](https://arxiv.org/abs/1904.09675). This method uses pre-trained
embedding and cosine similarity to calculate the candidate and reference sentence (each candidate matches only 
with one reference sentence). Furthermore, we wrote a separate function to graph the 
[Maximum Similarity matrix](https://github.com/Tiiiger/bert_score) between
the candidate sentence tokens, and the reference sentence tokens. bert_score only compares candidate with one reference, 
so we made the function to compute summary scores (mean, max, min) if there are multiple references.

```commandline
pip install bert_score
```

The higher the score, the more similar the candidate is with the references. We would use these evaluations with caution. 
While we do want our generation to assemble real lyrics, we do also want to
preserve novelty. Therefore, we do not hope the scores to be too high, which means the generated lyrics is too
close to the existing lyrics. 

### 4. Lexical diversity

Since we are generating lyrics, we do not want to have lyrics that have many repeated words or one that seems "boring".
Therefore, we used several methods to test if the lexica is diverse enough. The package we use is ```lexical_diversity```, 
which includes a few methods mentioned in paper [Evaluation of Text Generation: A Survey](https://arxiv.org/pdf/2006.14799.pdf).
* TTR (Type Token Ratio) - # of distinct tokens / # of total tokens. This is an intuitive measure, but has shortcomings. 
  We have a few variations of simple TTR: Log TTR, Mass TTR, MATTR(moving average), and MSTTR(Mean segmental TTR).
  
* HDD (Hyper-geometric Distribution D) - introduced in 
  [McCarthy & Jarvis, 2010] (https://link.springer.com/article/10.3758/BRM.42.2.381),
  which is a variation of Hyper-geometric Distribution (HD) method. According to the survey paper:
  "The HD is a discrete probability distribution that expresses the probability of k successes after drawing n items 
  from a finite population of size N containing m successes without replacement." However, as our experiment shows, this 
  method does not work well when the text is short. Therefore, we will likely not use this method.
  
* MLTD (Measure of lexical textual diversity) - introduced also in 
  [McCarthy & Jarvis, 2010] (https://link.springer.com/article/10.3758/BRM.42.2.381).
  We have a variation on this which is calculating the moving average, and bidirectional. 
  This again seems to work better on longer sentences, and thus we would likely not use it.
  
After experiments, it is likely better to use Simple TTR, Log TTR, MSTTR, or MATTR

### 5. Plagiarism check
We do not want to generate a sentence that is completely the same with the previous 

## Manual Evaluation
Criteria:
* How well does the generated sentence fit into the context of the previous sentence?
* How novel is the generation?
* Do you think it is easy to add a few words on top of the generation to fit into different length of the music segment?


# Instruction on running the file and sample output:
```
from evaluations import *

input_text1 = "My cat is a cute cat."
reference_test_list1 = ["He is also fat.", "He likes rats.", "I hope he can rap."]
generated_text1 = "He is not that fat."
scores1 = run_evaluations(input_text1, reference_test_list1, generated_text1)
print(scores1)
```

The output is:
```
{'grammar_score': 0.0, 
 'rhyme': True, 
 'bleu_score': 0.3872983346207417, 
 'rouge_score': 0.6666666617283951, 
 'bert_score': 0.9196805, 
 'ld_score': 1.0, 
 'plagiarism': False}
```

* The grammar score is 0 because there is no grammar mistake.
* The line of lyrics rhymes because the previous input last word is "cat", and the candidate last line is "fat".
* The ld_score is 1 because there is no repeated words.
* The plagiarism check is False because it is not a complete copy from any of the reference sentences.
* If we take mean for rouge, the score is 0.3055, which is similar to bleu score. However, we took max here for both
  rouge and bert_score because we think if it resembles relatively well with one of the references then it is fine. 








