{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 444,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'rouge'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-444-07398690d919>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcld3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mbaseline\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mevaluation\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/PycharmProjects/lyrics-generation/baseline.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mevaluation\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mstring\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpunctuation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/PycharmProjects/lyrics-generation/evaluation.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranslate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbleu_score\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msentence_bleu\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranslate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbleu_score\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSmoothingFunction\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mrouge\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mRouge\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mbert_score\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mscore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mbert_score\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mplot_example\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'rouge'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from langdetect import detect\n",
    "from sklearn.model_selection import train_test_split\n",
    "import cld3\n",
    "import math\n",
    "from baseline import *\n",
    "from evaluation import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 449,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_subset = train.iloc[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 450,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_subset.to_csv('data/train_subset.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 448,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Artist</th>\n",
       "      <th>Song</th>\n",
       "      <th>Genre</th>\n",
       "      <th>Language</th>\n",
       "      <th>Lyrics</th>\n",
       "      <th>reliable_lang</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>belle sebastian</td>\n",
       "      <td>legal man</td>\n",
       "      <td>Pop</td>\n",
       "      <td>en</td>\n",
       "      <td>L-O-V-E love, it's coming back, it's coming ba...</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>pete townshend</td>\n",
       "      <td>a friend is a friend</td>\n",
       "      <td>Rock</td>\n",
       "      <td>en</td>\n",
       "      <td>When eyes meet in silence\\nA pact can be made\\...</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>blossoms</td>\n",
       "      <td>love talk</td>\n",
       "      <td>Rock</td>\n",
       "      <td>en</td>\n",
       "      <td>my eyes align with you\\nyou're on my side\\nand...</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>bukka white</td>\n",
       "      <td>parchman farm blues</td>\n",
       "      <td>Folk</td>\n",
       "      <td>en</td>\n",
       "      <td>Judge gimme me life this morn'in\\nDown on Parc...</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>pat benatar</td>\n",
       "      <td>i won't</td>\n",
       "      <td>Rock</td>\n",
       "      <td>en</td>\n",
       "      <td>I was there when you cried like a baby\\nWhen y...</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Artist                  Song Genre Language  \\\n",
       "0  belle sebastian             legal man   Pop       en   \n",
       "1   pete townshend  a friend is a friend  Rock       en   \n",
       "2         blossoms             love talk  Rock       en   \n",
       "3      bukka white   parchman farm blues  Folk       en   \n",
       "4      pat benatar               i won't  Rock       en   \n",
       "\n",
       "                                              Lyrics reliable_lang  \n",
       "0  L-O-V-E love, it's coming back, it's coming ba...            en  \n",
       "1  When eyes meet in silence\\nA pact can be made\\...            en  \n",
       "2  my eyes align with you\\nyou're on my side\\nand...            en  \n",
       "3  Judge gimme me life this morn'in\\nDown on Parc...            en  \n",
       "4  I was there when you cried like a baby\\nWhen y...            en  "
      ]
     },
     "execution_count": 448,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = pd.read_csv(\"data/csv/train.csv\")\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"data/train.csv\")\n",
    "data = data[data['Language']=='en']\n",
    "data = data[data['Lyrics'].apply(lambda x: '\\n' in x)]\n",
    "data = data.drop_duplicates(subset=['Artist', 'Song', 'Genre'], ignore_index=True)\n",
    "data = data[data['Lyrics'].apply(lambda x: '---------' not in x)]\n",
    "data['Lyrics'] = data['Lyrics'].apply(lambda x: re.sub('\\[.*\\]', ' ', x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_en(line):\n",
    "    result = cld3.get_language(line)\n",
    "    lang = result[0]\n",
    "    reliable = result[2]\n",
    "    if reliable and lang == 'en':\n",
    "        return 'en'\n",
    "    else:\n",
    "        return 'other'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "192123"
      ]
     },
     "execution_count": 328,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['reliable_lang'] = data['Lyrics'].apply(check_en)\n",
    "data = data[data['reliable_lang']=='en']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = np.arange(len(data))\n",
    "train_index, test_index = train_test_split(indices, test_size = 0.1)\n",
    "train_index, dev_index = train_test_split(train_index, test_size=0.1)\n",
    "\n",
    "train_data = data.iloc[train_index].reset_index(drop=True)\n",
    "dev_data = data.iloc[dev_index].reset_index(drop=True)\n",
    "test_data = data.iloc[test_index].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.to_csv('data/csv/train.csv', index=False)\n",
    "dev_data.to_csv('data/csv/dev.csv', index=False)\n",
    "test_data.to_csv('data/csv/test.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Rock          96376\n",
       "Pop           70438\n",
       "Metal         15910\n",
       "Jazz          11701\n",
       "Folk           6922\n",
       "Indie          5592\n",
       "R&B            1572\n",
       "Electronic      447\n",
       "Name: Genre, dtype: int64"
      ]
     },
     "execution_count": 294,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['Genre'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/csv/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df['Genre']=='Pop']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Rock          8679\n",
       "Pop           6313\n",
       "Metal         1490\n",
       "Jazz          1035\n",
       "Folk           603\n",
       "Indie          497\n",
       "R&B            171\n",
       "Electronic      36\n",
       "Name: Genre, dtype: int64"
      ]
     },
     "execution_count": 280,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dev_data['Genre'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "jazz = data[data['Genre']=='Jazz'].reset_index(drop=True)\n",
    "indices = np.arange(len(jazz))\n",
    "train_index, test_index = train_test_split(indices, test_size = 0.1)\n",
    "train_index, dev_index = train_test_split(train_index, test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'data/train/jazz_train.txt'\n",
    "with open(path,'w', encoding='utf-8', errors='ignore') as f:\n",
    "    for i in train_index:\n",
    "#         f.write(\"ARTIST NAME: \"+jazz['Artist'].iloc[i]+'\\n')\n",
    "#         f.write(\"GENRE: \"+jazz['Genre'].iloc[i]+'\\n')\n",
    "        f.write(jazz['Lyrics'].iloc[i])\n",
    "        f.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "for g in list(data['Genre'].value_counts().index):\n",
    "    df = data[data['Genre']==g]\n",
    "    artist = np.array(df['Artist']).reshape(-1,1)\n",
    "    genre = np.array(df['Genre']).reshape(-1,1)\n",
    "    lyrics = np.array(df['Lyrics']).reshape(-1,1)\n",
    "\n",
    "    out = np.hstack((artist, genre, lyrics))\n",
    "    file_path = 'data/'+g+'_train.npy'\n",
    "    np.save(file_path, out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "artist = np.array(rnb['Artist']).reshape(-1,1)\n",
    "genre = np.array(rnb['Genre']).reshape(-1,1)\n",
    "lyrics = np.array(rnb['Lyrics']).reshape(-1,1)\n",
    "\n",
    "rnb_out = np.hstack((artist, genre, lyrics))\n",
    "np.save('data/rnb_train.npy', rnb_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'en'"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "detect(rnb['Lyrics'].iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/haowei/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "rnb['detect_lang'] = rnb['Lyrics'].apply(lambda x: detect(x[:50]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Rock          103721\n",
       "Pop            83620\n",
       "Metal          17207\n",
       "Jazz           11760\n",
       "Folk            7023\n",
       "Indie           6033\n",
       "R&B             1575\n",
       "Electronic       462\n",
       "Name: Genre, dtype: int64"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['Genre'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " \n",
      "\n",
      "so many people \n",
      "I know only a few \n",
      "yes I may say that I love this man \n",
      "and that man \n",
      "but what keeps me from loving you? \n",
      "date of birth  geography \n",
      "the color of my skin  ideology \n",
      "you got ten fingers  two legs  one nose \n",
      "like me \n",
      "just like me \n",
      "and it's as simple as that \n",
      "you see \n",
      "and if I don't know who to love \n",
      "I love them all \n",
      "and if I don't know who to trust \n",
      "I trust them all \n",
      "and if I don't know who to kill \n",
      "I may kill myself instead \n",
      "from the mouth of a baby \n",
      "will come the world-saving words \n",
      "that will save us all \n",
      "and from the lungs of a child \n",
      "will come the everlasting breath of god \n",
      "increasing peace and honesty \n",
      "and not carrying on despite of me \n",
      "don't you know \n",
      "this ain't about no race, no creed \n",
      "no race, no creed \n",
      "and it's as simple as that \n",
      "you see \n",
      "and if I don't know who to love \n",
      "I love them all \n",
      "and if I don't know who to trust \n",
      "I trust them all \n",
      "and if I don't know who to kill \n",
      "no suicide \n",
      "I'm already dead \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(rnb.iloc[20]['Lyrics'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Rock          70156\n",
       "Pop           53050\n",
       "Metal         11728\n",
       "Jazz           9457\n",
       "Folk           5143\n",
       "Indie          4475\n",
       "R&B            1258\n",
       "Electronic      352\n",
       "Name: Genre, dtype: int64"
      ]
     },
     "execution_count": 398,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df['Genre'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 441,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_ngram_model(model_class, path, n=2, k=0, type='csv', genre=None):\n",
    "    ''' Creates and returns a new n-gram model trained on path file '''\n",
    "    \"\"\"\n",
    "    path: numpy file of dimension nx3, columns: artist, genre, lyrics\n",
    "    \n",
    "    return: updated model of model_class\n",
    "    \"\"\"\n",
    "    num_iterations = 100  # number of samples to draw\n",
    "    length = 20  # how many lines in one sample\n",
    "\n",
    "    model = model_class(n, k)\n",
    "#     if type == 'txt':\n",
    "#         with open(path, encoding='utf-8', errors='ignore') as f:\n",
    "#             lyrics = f.read()\n",
    "#     else:\n",
    "#         lyrics = csv_to_text(path, genre)\n",
    "\n",
    "#     for i in range(num_iterations):\n",
    "#         temp = lyrics_chunks(lyrics, length)\n",
    "#         model.update(temp)\n",
    "    df = pd.read_csv(path)\n",
    "    df = df[df['Genre']==genre]\n",
    "    for i in range(len(df)):\n",
    "        lyric = df.iloc[i]['Lyrics']\n",
    "        model.update(lyric)\n",
    "\n",
    "    return model\n",
    "\n",
    "def start_pad(n):\n",
    "    ''' Returns a padding string of length n to append to the front of text\n",
    "        as a pre-processing step to building n-grams '''\n",
    "    return '~' * n\n",
    "\n",
    "def ngrams(n, text):\n",
    "    ''' Returns the ngrams of the text as tuples where the first element is\n",
    "        the length-n context and the second is the character '''\n",
    "    adjusted_txt = start_pad(n) + text\n",
    "    gramlist = []\n",
    "    for i in range(n, len(adjusted_txt)):\n",
    "        ngram = (adjusted_txt[i-n:i], adjusted_txt[i])\n",
    "        gramlist.append(ngram)\n",
    "    return gramlist\n",
    "\n",
    "def csv_to_text(path, genre=None):\n",
    "    ''' Takes in a csv file and return the lyrics concatenated into one text file'''\n",
    "    df = pd.read_csv(path)\n",
    "    if genre:\n",
    "        df = df[df['Genre']==genre]\n",
    "    lyrics = '\\n'.join(list(df['Lyrics']))\n",
    "    return lyrics\n",
    "\n",
    "def lyrics_chunks(lyrics, length):\n",
    "    \"\"\"\n",
    "    randomly samples lyrics chunks of specified length\n",
    "\n",
    "    :param lyrics: str, entire lyrics corpus\n",
    "    :param length: int, number of lines in one lyrics chunks\n",
    "    :return: str, lyric lines joined by new line\n",
    "    \"\"\"\n",
    "    lyrics_lines = lyrics.split('\\n')\n",
    "    start_idx = random.randint(0, len(lyrics_lines)-1)\n",
    "    end_idx = min(start_idx+length, len(lyrics_lines))\n",
    "    selected_lyrics = lyrics_lines[start_idx:end_idx]\n",
    "    return '\\n'.join(selected_lyrics)\n",
    "\n",
    "################################################################################\n",
    "# Model\n",
    "################################################################################\n",
    "\n",
    "class NgramModel(object):\n",
    "    ''' A basic n-gram model using add-k smoothing '''\n",
    "\n",
    "    def __init__(self, n, k):\n",
    "        self.n = n\n",
    "        self.k = k\n",
    "        self.vocabs = []\n",
    "        self.context_list = {}\n",
    "        self.context_count = {}\n",
    "\n",
    "    def get_vocab(self):\n",
    "        ''' Returns the set of characters in the vocab '''\n",
    "        return set(''.join(self.vocabs))\n",
    "\n",
    "    def update(self, text):\n",
    "        ''' Updates the model n-grams based on text '''\n",
    "        self.vocabs.append(text)\n",
    "        # self.sorted_vocabs = list(set(self.vocabs))\n",
    "        # self.sorted_vocabs.sort()\n",
    "        gramlist = ngrams(self.n, text)\n",
    "        for pair in gramlist:\n",
    "            context = pair[0]\n",
    "            char = pair[1]\n",
    "            if context in self.context_list: # context previously showed up\n",
    "                self.context_count[context]+=1\n",
    "                if char in self.context_list[context]: # combination previously showed up\n",
    "                    self.context_list[context][char]+=1\n",
    "                else:\n",
    "                    self.context_list[context][char]=1\n",
    "            else:\n",
    "                self.context_count[context]=1\n",
    "                self.context_list[context] = {char: 1}\n",
    "\n",
    "    def prob(self, context, char, lambdas=None):\n",
    "        ''' Returns the probability of char appearing after context '''\n",
    "        if context not in self.context_list:\n",
    "            return 1/len(self.get_vocab())\n",
    "\n",
    "        # if char not in self.context_list[context]:\n",
    "        #     return 0.0\n",
    "        # else:\n",
    "        #     return self.context_list[context][char]/self.context_count[context]\n",
    "        count = 0\n",
    "        if char in self.context_list[context]:\n",
    "            count = self.context_list[context][char]\n",
    "        p = (count+self.k)/(self.context_count[context]+self.k*len(self.get_vocab()))\n",
    "        return p\n",
    "\n",
    "    def random_char(self, context):\n",
    "        ''' Returns a random character based on the given context and the\n",
    "            n-grams learned by this model '''\n",
    "        vocabs = list(self.get_vocab())\n",
    "        vocabs.sort()\n",
    "        total_prob = 0\n",
    "        r = random.random()\n",
    "        for i in range(len(vocabs)):\n",
    "            total_prob += self.prob(context, vocabs[i])\n",
    "            if total_prob > r:\n",
    "                return vocabs[i]\n",
    "        return vocabs[-1]\n",
    "\n",
    "    def random_text(self, length, context=None):\n",
    "        ''' Returns text of the specified character length based on the\n",
    "            n-grams learned by this model '''\n",
    "        if context == None:\n",
    "            context = start_pad(self.n)\n",
    "        else:\n",
    "            context = context[-self.n:]\n",
    "        text = ''\n",
    "        for i in range(length):\n",
    "            char = self.random_char(context)\n",
    "            text += char\n",
    "            context = context[1:]+char\n",
    "        return text\n",
    "\n",
    "    def random_lines(self, num_lines, context=None):\n",
    "        ''' Returns specified number of lines based on learned n-grams model'''\n",
    "        if context == None:\n",
    "            context = start_pad(self.n)\n",
    "        else:\n",
    "            context = context[-self.n:]\n",
    "        text = ''\n",
    "        count = 0\n",
    "        while True:\n",
    "            char = self.random_char(context)\n",
    "            if char == '\\n':\n",
    "                count += 1\n",
    "                if count == num_lines:\n",
    "                    break\n",
    "            text += char\n",
    "            context = context[1:] + char\n",
    "        return text\n",
    "    \n",
    "    def perplexity(self, text,lambdas=None):\n",
    "        ''' Returns the perplexity of text based on the n-grams learned by\n",
    "            this model '''\n",
    "        padded_text = start_pad(self.n) + text\n",
    "        context = start_pad(self.n)\n",
    "        log_pp = 0\n",
    "        for i in range(self.n, len(padded_text)):\n",
    "            p = self.prob(context, padded_text[i])\n",
    "            if p == 0:\n",
    "                return float('inf')\n",
    "            log_pp += math.log(p)\n",
    "            context = context[1:]+padded_text[i]\n",
    "        log_pp = log_pp/(-len(text))\n",
    "        pp = math.exp(log_pp)\n",
    "        return pp\n",
    "    \n",
    "    def gen_next_sent(self, input_sent):\n",
    "        ''' Returns the next sentence prediction based on input sentence '''\n",
    "        input_sent = start_pad(self.n) + input_sent + '\\n'\n",
    "        context = input_sent[-self.n:]\n",
    "        next_sent = ''\n",
    "        while True:\n",
    "            char = self.random_char(context)\n",
    "            if char != '\\n':\n",
    "                next_sent += char\n",
    "                context = context[1:] + char\n",
    "            else:\n",
    "                break\n",
    "        return next_sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 442,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnb = create_ngram_model(NgramModel, 'data/csv/train.csv', 4,0.0000001,genre='R&B')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 437,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnb6 = create_ngram_model(NgramModel, 'data/csv/train.csv', 6,0.0000001,genre='R&B')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 443,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I will in my loaded came'"
      ]
     },
     "execution_count": 443,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnb.gen_next_sent('I like')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 432,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sweet apology\n",
      "Then bad line of these world whiskey and sleep 'cause you're nothings that's wrong (shout\n",
      "You thing's the wet myself out for You\n",
      "\n",
      "My little more electric with dreamed Sadie\n",
      "Yeah, you weren't wear I've that bring is up\n",
      "Rider of us don't be a funk that family.\n",
      "The airport to hide you\n",
      "(Verse)\n",
      "I wrote and stuffed in the way I hear\n"
     ]
    }
   ],
   "source": [
    "indie = create_ngram_model(NgramModel, 'data/csv/train.csv', 5,0.0000001,genre='Indie')\n",
    "print(indie.random_lines(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 433,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Greasy here \n",
      "What a Swedish generation?\n",
      "\n",
      "Am I repeated)\n",
      "I luv ya (layered little now\n",
      "Oh, you must be having now \n",
      "I'll take my past if for the ooooooooooo\n",
      "there's a lesson to the need to take\n",
      "we're both breadcrumbs\n",
      "And you could ask what's all you\n"
     ]
    }
   ],
   "source": [
    "indie6 = create_ngram_model(NgramModel, 'data/csv/train.csv', 6,0.0000001,genre='Indie')\n",
    "print(indie6.random_lines(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 427,
   "metadata": {},
   "outputs": [],
   "source": [
    "def avg_perplexity(model, test_df, model_type='ngram', genre=None):\n",
    "    \"\"\"\n",
    "    computes perplexity of unseen text using given model\n",
    "\n",
    "    :param model: learned model\n",
    "    :param test_df:\n",
    "    :param model_type: 'ngram' or 'neural', default is 'ngram'\n",
    "    :return: average perplexity score from 20 songs\n",
    "    \"\"\"\n",
    "    tot_pp = 0\n",
    "    num_samp = 150\n",
    "    \n",
    "    if genre:\n",
    "        test_df = test_df[test_df['Genre']==genre]\n",
    "    \n",
    "    if model_type == 'ngram':\n",
    "        # select 20 random songs from test data and compute perplexity\n",
    "        for i in range(num_samp):\n",
    "            idx = random.randint(0,len(test_df)-1)\n",
    "            lyric = test_df.iloc[idx]['Lyrics']\n",
    "            tot_pp += model.perplexity(lyric)\n",
    "\n",
    "#     if model_type == 'neural':\n",
    "#         for i in range(num_samp):\n",
    "#             idx = random.randint(len(test_df))\n",
    "#             lyric = test_df.iloc[idx]['Lyrics']\n",
    "#             tot_pp += nn_perplexity(lyric, model)\n",
    "\n",
    "    return tot_pp / num_samp\n",
    "\n",
    "# def nn_perplexity(val_text, model):\n",
    "#     input = char_tensor(val_text[:-1])\n",
    "#     target = char_tensor(val_text[1:])\n",
    "#     hidden = model.init_hidden()\n",
    "#     criterion = nn.CrossEntropyLoss()\n",
    "#     loss = 0\n",
    "#     for i in range(len(val_text)-1):\n",
    "#         output, hidden = model(input[i], hidden)\n",
    "#         loss += criterion(output, target[i].view(-1))\n",
    "#     avg_loss = loss.item()/(len(val_text)-1)\n",
    "#     return np.exp(avg_loss)\n",
    "\n",
    "# def char_tensor(string):\n",
    "#     tensor = torch.zeros(len(string)).long()\n",
    "#     for c in range(len(string)):\n",
    "#         tensor[c] = all_characters.index(string[c])\n",
    "#     return Variable(tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.read_csv('data/csv/test.csv')\n",
    "train_df = pd.read_csv('data/csv/train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = random.randint(0,len(test_df)-1)\n",
    "lyric = test_df.iloc[idx]['Lyrics']\n",
    "# tot_pp += model.perplexity(lyric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 430,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-430-9fbaf79b59e8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mavg_perplexity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrnb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_df\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'ngram'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'R&B'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mavg_perplexity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrnb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_df\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'ngram'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Indie'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-427-9e67780d673c>\u001b[0m in \u001b[0;36mavg_perplexity\u001b[0;34m(model, test_df, model_type, genre)\u001b[0m\n\u001b[1;32m     19\u001b[0m             \u001b[0midx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m             \u001b[0mlyric\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Lyrics'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m             \u001b[0mtot_pp\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mperplexity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlyric\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;31m#     if model_type == 'neural':\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-399-f934dd78def6>\u001b[0m in \u001b[0;36mperplexity\u001b[0;34m(self, text, lambdas)\u001b[0m\n\u001b[1;32m    168\u001b[0m         \u001b[0mlog_pp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpadded_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 170\u001b[0;31m             \u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadded_text\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    171\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mp\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'inf'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-399-f934dd78def6>\u001b[0m in \u001b[0;36mprob\u001b[0;34m(self, context, char, lambdas)\u001b[0m\n\u001b[1;32m    113\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mchar\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m             \u001b[0mcount\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mchar\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m         \u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext_count\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_vocab\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-399-f934dd78def6>\u001b[0m in \u001b[0;36mget_vocab\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     80\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_vocab\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m         \u001b[0;34m''' Returns the set of characters in the vocab '''\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 82\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocabs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "print(avg_perplexity(rnb, test_df,'ngram', 'R&B'))\n",
    "print(avg_perplexity(rnb, test_df,'ngram', 'Indie'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NgramModel(5,0)\n",
    "for i in range(5000):\n",
    "    model.update(lyrics[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['~~~~~'])"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.context_list.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aIxPtb[KYIaP\n",
      "F.dAFzEkN[FSTVDlnnoE,IfqlshVHmz]Js-pbPCWDPxcK-SYyIWL1JqUyrEyd\n",
      "H1DOWS1c'mf:s\n",
      "x'jygrbvrez\n"
     ]
    }
   ],
   "source": [
    "print(model.random_text(100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
